{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import json \n",
    "import math \n",
    "import numpy as np \n",
    "\n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n",
    "import torch.optim as optim \n",
    "import torch.utils.data as data\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "import random\n",
    "\n",
    "import logging "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append(\"../utils\")\n",
    "\n",
    "from misc import seed_everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = \"./data\"\n",
    "CHECKPOINT_PATH = \"./checkpoint\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "@dataclass\n",
    "class Params:\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    batch_size = 128\n",
    "    img_shape = (1,28,28)\n",
    "    lr = 1e-4\n",
    "    alpha = 0.1\n",
    "    beta1=0.0\n",
    "    beta2 = 0.99\n",
    "\n",
    "params = Params()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
    ")\n",
    "\n",
    "\n",
    "train_dataset = datasets.MNIST(root=DATASET_PATH, train=True, download=True, transform=transform)\n",
    "valid_dataset = datasets.MNIST(root=DATASET_PATH, train=False, download=True, transform=transform)\n",
    "\n",
    "\n",
    "train_dataloader = data.DataLoader(train_dataset, batch_size=64, shuffle=True, drop_last=True, num_workers=4, pin_memory=True)\n",
    "valid_dataloader = data.DataLoader(valid_dataset, batch_size=128, shuffle=False, drop_last=False, num_workers=1, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Swish(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x * torch.sigmoid(x)\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, hidden_features = 32, out_dim = 1, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        c_hid1 = hidden_features // 2\n",
    "        c_hid2 = hidden_features\n",
    "        c_hid3 = hidden_features * 2\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv2d(1, c_hid1, kernel_size=5, stride=2, padding=4),\n",
    "            Swish(),\n",
    "            nn.Conv2d(c_hid1, c_hid2, kernel_size=3, stride=2, padding=1),\n",
    "            Swish(),\n",
    "            nn.Conv2d(c_hid2, c_hid3, kernel_size=3, stride=2, padding=1),\n",
    "            Swish(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(c_hid3 *4, c_hid3),\n",
    "            Swish(),\n",
    "            nn.Linear(c_hid3, out_dim)\n",
    "        )\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x).squeeze(dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Sampler Buffer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampler:\n",
    "    def __init__(self, model, img_shape, sample_size, max_len = 8192):\n",
    "        super().__init__()\n",
    "        self.model = model \n",
    "        self.img_shape = img_shape \n",
    "        self.sample_size = sample_size \n",
    "        self.max_len = max_len \n",
    "        self.examples = [\n",
    "            (torch.rand((1,) + img_shape) * 2 - 1)\n",
    "            for _ in range(self.sample_size)\n",
    "        ]\n",
    "\n",
    "    def sample_new_exmps(self, steps = 60, step_size = 10):\n",
    "        n_new = np.random.binomial(self.sample_size, 0.05)\n",
    "        rand_imgs = torch.rand((n_new,) + self.img_shape) * 2 - 1 \n",
    "        old_imgs = torch.cat(random.choices(self.examples, k=self.sample_size - n_new), dim=0)\n",
    "        inp_imgs = torch.cat([rand_imgs, old_imgs], dim=0).detach().to(params.device)\n",
    "\n",
    "        # Perform MCMC sampling\n",
    "        inp_imgs = Sampler.generate_samples(self.model, inp_imgs, steps, step_size)\n",
    "\n",
    "        # Add new images to the buffer and remove old ones if needed\n",
    "        self.examples = list(\n",
    "            inp_imgs.to(torch.device('cpu')).chunk(self.sample_size, dim=0)\n",
    "        ) + self.examples\n",
    "        self.examples = self.examples[:self.max_len]\n",
    "        return inp_imgs\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_samples(model, inp_imgs, steps=60, step_size = 10, return_img_per_step=False):\n",
    "\n",
    "        is_training = model.training \n",
    "        model.eval()\n",
    "        for p in model.parameters():\n",
    "            p.requires_grad = False \n",
    "        inp_imgs.requires_grad = True  # Gradient with respect to the input image\n",
    "\n",
    "        # Enable gradient calculation if not already the case\n",
    "        has_gradients_enabled = torch.is_grad_enabled()\n",
    "        torch.set_grad_enabled(True )\n",
    "\n",
    "        noise = torch.randn(inp_imgs.shape, device = params.device)\n",
    "        imgs_per_step = []\n",
    "\n",
    "        # Loop over K (steps)\n",
    "        for _ in range(steps):\n",
    "            noise.normal_(0, 0.005)\n",
    "            inp_imgs.data.add_(noise.data)\n",
    "            inp_imgs.data.clamp_(min=-1.0, max=1.0)\n",
    "\n",
    "            out_imgs = -model(inp_imgs) # -E(x)\n",
    "            out_imgs.sum().backward()\n",
    "            inp_imgs.grad.data.clamp_(-0.03, 0.03)\n",
    "\n",
    "            # Apply gradients to current samples\n",
    "            inp_imgs.data.data_(-step_size * inp_imgs.grad.data)\n",
    "            inp_imgs.grad.detach_()\n",
    "            inp_imgs.grad.zero_()\n",
    "            inp_imgs.data.clamp_(min=1.0, max=1.0)\n",
    "\n",
    "            if return_img_per_step:\n",
    "                imgs_per_step.append(inp_imgs.clone.detach())\n",
    "\n",
    "        for p in model.parameters():\n",
    "            p.requires_grad = True \n",
    "        model.train(is_training) \n",
    "\n",
    "        # Reset gradient calculation to setting before this function\n",
    "        torch.set_grad_enabled(has_gradients_enabled)\n",
    "        if return_img_per_step:\n",
    "            return torch.stack(imgs_per_step, dim=0)\n",
    "        else:\n",
    "            return inp_imgs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()\n",
    "sampler = Sampler(net, img_shape=params.img_shape, sample_size=params.batch_size)\n",
    "example_input_array = torch.zeros(1, *params.img_shape)\n",
    "optimizer = optim.Adam(net.parameters(), lr=params.lr, betas=(params.beta1, params.beta2))\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.97)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerateCallback:\n",
    "    def __init__(self, batch_size =8, vis_steps = 8, num_steps = 256, every_n_epochs = 5):\n",
    "        self.batch_size = batch_size \n",
    "        self.vis_steps = vis_steps \n",
    "        self.num_steps=num_steps\n",
    "        self.every_n_epochs = every_n_epochs\n",
    "    \n",
    "    def on_epoch_end(self, trainer , model):\n",
    "        if trainer.current_epoch % self.every_n_epochs == 0:\n",
    "            # Generate images \n",
    "            imgs_per_step = self.generate_imgs(model)\n",
    "            \n",
    "            # for i in range(imgs_per_step.shape[1]):\n",
    "            #     step_size = self.num_steps // self.vis_steps \n",
    "            #     imgs_to_plot = imgs_per_step[step_size -1 :: step_size, i]\n",
    "            #     grid = torchvision.utils.make_grid(imgs_to_plot, nrow=imgs_to_plot.shape[0], normalize=True, range=(-1, 1))\n",
    "                \n",
    "    \n",
    "    def generate_imgs(self, model):\n",
    "        model.eval()\n",
    "        start_imgs = torch.rand((self.batch_size,) + params.img_shape).to(params.device)\n",
    "        start_imgs = start_imgs * 2 - 1 \n",
    "        torch.set_grad_enabled(True)\n",
    "        imgs_per_step = Sampler.generate_samples(model, start_imgs, steps=self.num_steps, step_size = 10, return_img_per_step = True)\n",
    "        torch.set_grad_enabled(False)\n",
    "        model.train()\n",
    "        return imgs_per_step\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SamplerCallbacl():\n",
    "    def __init__(self, num_imgs=32, every_n_epochs =5):\n",
    "        self.num_imgs = num_imgs \n",
    "        self.every_n_epochs = every_n_epochs \n",
    "\n",
    "    def on_epoch_end(self, trainer, sampler):\n",
    "        if trainer.current_epoch % self.every_n_epochs == 0:\n",
    "            exmp_imgs = torch.cat(random.choices(sampler.examples, k=self.num_imgs), dim=0)\n",
    "            grid = torchvision.utils.make_grid(\n",
    "                exmp_imgs, nrow=4, normalize=True, range=(-1, 1)\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutlierCallback():\n",
    "\n",
    "    def __init__(self, batch_size=1024):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def on_epoch_end(self, trainer, model):\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            rand_imgs = torch.rand(\n",
    "                (self.batch_size,) + params.img_shape\n",
    "            ).to(params.device)\n",
    "            rand_imgs = rand_imgs * 2 - 1.0\n",
    "            rand_out = model(rand_imgs).mean()\n",
    "            model.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Trainer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trainer import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetTrainer(Trainer):\n",
    "    def __init__(self, model, criterion, optimizer, lr_scheduler, metrics, dataloaders,  params,):\n",
    "        \n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer \n",
    "        self.lr_scheduler = lr_scheduler \n",
    "        self.metrics = metrics \n",
    "        self.dataloaders = dataloaders\n",
    "        \n",
    "        self.train_losses = []\n",
    "        self.valid_losses = []\n",
    "\n",
    "    def train_one_epoch(self):\n",
    "        \n",
    "        self.model.train()\n",
    "        loss_avg  = AverageMeter()\n",
    "        \n",
    "        train_dataloade = self.dataloaders['train']\n",
    "        with tqdm(total=len(train_dataloader)) as t:\n",
    "            for real_imgs, _ in train_dataloader:\n",
    "                small_noise = torch.randn_like(real_imgs) * 0.005\n",
    "                real_imgs.add_(small_noise).clamp_(min=-1.0, max=1.0)\n",
    "                \n",
    "                \n",
    "                # Obtain samples \n",
    "                fake_imgs = self.sampler.sample_new_exmps(steps=60, step_size = 10)\n",
    "                \n",
    "                # Predict energy score for all images\n",
    "                inp_imgs = torch.cat([real_imgs, fake_imgs], dim=0)\n",
    "                real_out, fake_out = self.model(inp_imgs).chunk(2, dim=0)\n",
    "                \n",
    "                reg_loss = self.params.alpha * (real_out ** 2 + fake_out ** 2).mean()\n",
    "                cdiv_loss = fake_out.mean() - real_out.mean()\n",
    "                loss = reg_loss + cdiv_loss \n",
    "                \n",
    "                \n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                loss_avg.update(loss.item())\n",
    "                t.set_postfix(loss=\"{:05.3f}\".format(loss_avg()))\n",
    "                t.update()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def valid_one_epoch(self):\n",
    "        self.model.eval()\n",
    "        \n",
    "        valid_dataloader = self.dataloaders['valid']\n",
    "        \n",
    "        loss = AverageMeter()\n",
    "        \n",
    "        for real_imgs, _ in valid_dataloader:\n",
    "            fake_imgs = torch.rand_like(real_imgs) * 2 - 1 \n",
    "            \n",
    "            inp_imgs = torch.cat([real_imgs, fake_imgs], dim=0)\n",
    "            real_out, fake_out = self.model(inp_imgs).chunk(2, dim=0)\n",
    "            \n",
    "            cdiv = fake_out.mean() - real_out.mean()\n",
    "            \n",
    "            loss.update(cdiv.item())\n",
    "            \n",
    "        \n",
    "\n",
    "    def train(self):\n",
    "        for epoch in self.params.epochs:\n",
    "            logging.info(f\"Epoch {epoch + 1} / {self.params.epochs}\")\n",
    "            print(f\"Epoch {epoch + 1} / {self.params.epochs}\")\n",
    "            \n",
    "            self.train_one_epoch()\n",
    "            self.valid_one_epoch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
