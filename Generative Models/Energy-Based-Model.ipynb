{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import json \n",
    "import math \n",
    "import numpy as np \n",
    "\n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n",
    "import torch.optim as optim \n",
    "import torch.utils.data as data\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "import random\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append(\"../utils\")\n",
    "\n",
    "from misc import seed_everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = \"./data\"\n",
    "CHECKPOINT_PATH = \"./checkpoint\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "@dataclass\n",
    "class Params:\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    batch_size = 128\n",
    "    img_shape = (1,28,28)\n",
    "    lr = 1e-4\n",
    "    alpha = 0.1\n",
    "    beta1=0.0\n",
    "    beta2 = 0.99\n",
    "\n",
    "params = Params()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9912422/9912422 [00:04<00:00, 2246570.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28881/28881 [00:00<00:00, 114743.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1648877/1648877 [00:03<00:00, 523168.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4542/4542 [00:00<00:00, 1348232.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
    ")\n",
    "\n",
    "\n",
    "train_dataset = datasets.MNIST(root=DATASET_PATH, train=True, download=True, transform=transform)\n",
    "valid_dataset = datasets.MNIST(root=DATASET_PATH, train=False, download=True, transform=transform)\n",
    "\n",
    "\n",
    "train_dataloader = data.DataLoader(train_dataset, batch_size=64, shuffle=True, drop_last=True, num_workers=4, pin_memory=True)\n",
    "valid_dataloader = data.DataLoader(valid_dataset, batch_size=128, shuffle=False, drop_last=False, num_workers=1, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Swish(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x * torch.sigmoid(x)\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, hidden_features = 32, out_dim = 1, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        c_hid1 = hidden_features // 2\n",
    "        c_hid2 = hidden_features\n",
    "        c_hid3 = hidden_features * 2\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv2d(1, c_hid1, kernel_size=5, stride=2, padding=4),\n",
    "            Swish(),\n",
    "            nn.Conv2d(c_hid1, c_hid2, kernel_size=3, stride=2, padding=1),\n",
    "            Swish(),\n",
    "            nn.Conv2d(c_hid2, c_hid3, kernel_size=3, stride=2, padding=1),\n",
    "            Swish(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(c_hid3 *4, c_hid3),\n",
    "            Swish(),\n",
    "            nn.Linear(c_hid3, out_dim)\n",
    "        )\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x).squeeze(dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Sampler Buffer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampler:\n",
    "    def __init__(self, model, img_shape, sample_size, max_len = 8192):\n",
    "        super().__init__()\n",
    "        self.model = model \n",
    "        self.img_shape = img_shape \n",
    "        self.sample_size = sample_size \n",
    "        self.max_len = max_len \n",
    "        self.examples = [\n",
    "            (torch.rand((1,) + img_shape) * 2 - 1)\n",
    "            for _ in range(self.sample_size)\n",
    "        ]\n",
    "\n",
    "    def sample_new_exmps(self, steps = 60, step_size = 10):\n",
    "        n_new = np.random.binomial(self.sample_size, 0.05)\n",
    "        rand_imgs = torch.rand((n_new,) + self.img_shape) * 2 - 1 \n",
    "        old_imgs = torch.cat(random.choices(self.examples, k=self.sample_size - n_new), dim=0)\n",
    "        inp_imgs = torch.cat([rand_imgs, old_imgs], dim=0).detach().to(params.device)\n",
    "\n",
    "        # Perform MCMC sampling\n",
    "        inp_imgs = Sampler.generate_samples(self.model, inp_imgs, steps, step_size)\n",
    "\n",
    "        # Add new images to the buffer and remove old ones if needed\n",
    "        self.examples = list(\n",
    "            inp_imgs.to(torch.device('cpu')).chunk(self.sample_size, dim=0)\n",
    "        ) + self.examples\n",
    "        self.examples = self.examples[:self.max_len]\n",
    "        return inp_imgs\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_samples(model, inp_imgs, steps=60, step_size = 10, return_img_per_step=False):\n",
    "\n",
    "        is_training = model.training \n",
    "        model.eval()\n",
    "        for p in model.parameters():\n",
    "            p.requires_grad = False \n",
    "        inp_imgs.requires_grad = True  # Gradient with respect to the input image\n",
    "\n",
    "        # Enable gradient calculation if not already the case\n",
    "        has_gradients_enabled = torch.is_grad_enabled()\n",
    "        torch.set_grad_enabled(True )\n",
    "\n",
    "        noise = torch.randn(inp_imgs.shape, device = params.device)\n",
    "        imgs_per_step = []\n",
    "\n",
    "        # Loop over K (steps)\n",
    "        for _ in range(steps):\n",
    "            noise.normal_(0, 0.005)\n",
    "            inp_imgs.data.add_(noise.data)\n",
    "            inp_imgs.data.clamp_(min=-1.0, max=1.0)\n",
    "\n",
    "            out_imgs = -model(inp_imgs) # -E(x)\n",
    "            out_imgs.sum().backward()\n",
    "            inp_imgs.grad.data.clamp_(-0.03, 0.03)\n",
    "\n",
    "            # Apply gradients to current samples\n",
    "            inp_imgs.data.data_(-step_size * inp_imgs.grad.data)\n",
    "            inp_imgs.grad.detach_()\n",
    "            inp_imgs.grad.zero_()\n",
    "            inp_imgs.data.clamp_(min=1.0, max=1.0)\n",
    "\n",
    "            if return_img_per_step:\n",
    "                imgs_per_step.append(inp_imgs.clone.detach())\n",
    "\n",
    "        for p in model.parameters():\n",
    "            p.requires_grad = True \n",
    "        model.train(is_training) \n",
    "\n",
    "        # Reset gradient calculation to setting before this function\n",
    "        torch.set_grad_enabled(has_gradients_enabled)\n",
    "        if return_img_per_step:\n",
    "            return torch.stack(imgs_per_step, dim=0)\n",
    "        else:\n",
    "            return inp_imgs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()\n",
    "sampler = Sampler(net, img_shape=params.img_shape, sample_size=params.batch_size)\n",
    "example_input_array = torch.zeros(1, *params.img_shape)\n",
    "optimizer = optim.Adam(net.parameters(), lr=params.lr, betas=(params.beta1, params.beta2))\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.97)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerateCallback:\n",
    "    def __init__(self, batch_size =8, vis_steps = 8, num_steps = 256, every_n_epochs = 5):\n",
    "        self.batch_size = batch_size \n",
    "        self.vis_steps = vis_steps \n",
    "        self.num_steps=num_steps\n",
    "        self.every_n_epochs = every_n_epochs\n",
    "    \n",
    "    def on_epoch_end(self, trainer , model):\n",
    "        if trainer.current_epoch % self.every_n_epochs == 0:\n",
    "            # Generate images \n",
    "            imgs_per_step = self.generate_imgs(model)\n",
    "            \n",
    "            # for i in range(imgs_per_step.shape[1]):\n",
    "            #     step_size = self.num_steps // self.vis_steps \n",
    "            #     imgs_to_plot = imgs_per_step[step_size -1 :: step_size, i]\n",
    "            #     grid = torchvision.utils.make_grid(imgs_to_plot, nrow=imgs_to_plot.shape[0], normalize=True, range=(-1, 1))\n",
    "                \n",
    "    \n",
    "    def generate_imgs(self, model):\n",
    "        model.eval()\n",
    "        start_imgs = torch.rand((self.batch_size,) + params.img_shape).to(params.device)\n",
    "        start_imgs = start_imgs * 2 - 1 \n",
    "        torch.set_grad_enabled(True)\n",
    "        imgs_per_step = Sampler.generate_samples(model, start_imgs, steps=self.num_steps, step_size = 10, return_img_per_step = True)\n",
    "        torch.set_grad_enabled(False)\n",
    "        model.train()\n",
    "        return imgs_per_step\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SamplerCallbacl():\n",
    "    def __init__(self, num_imgs=32, every_n_epochs =5):\n",
    "        self.num_imgs = num_imgs \n",
    "        self.every_n_epochs = every_n_epochs \n",
    "\n",
    "    def on_epoch_end(self, trainer, sampler):\n",
    "        if trainer.current_epoch % self.every_n_epochs == 0:\n",
    "            exmp_imgs = torch.cat(random.choices(sampler.examples, k=self.num_imgs), dim=0)\n",
    "            grid = torchvision.utils.make_grid(\n",
    "                exmp_imgs, nrow=4, normalize=True, range=(-1, 1)\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutlierCallback():\n",
    "\n",
    "    def __init__(self, batch_size=1024):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def on_epoch_end(self, trainer, model):\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            rand_imgs = torch.rand(\n",
    "                (self.batch_size,) + params.img_shape\n",
    "            ).to(params.device)\n",
    "            rand_imgs = rand_imgs * 2 - 1.0\n",
    "            rand_out = model(rand_imgs).mean()\n",
    "            model.train()\n",
    "\n",
    "        trainer.logger.experiment.add_scalar(\n",
    "            \"rand_out\", rand_out, global_step=trainer.current_epoch\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
